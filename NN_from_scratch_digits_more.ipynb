{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the code from chapter 18 of the book, \"Data Science from Scratch\" by Joel Grus, available on [github][1].\n",
    "\n",
    "[1]: https://github.com/joelgrus/data-science-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def argmax(l):\n",
    "    return l.index(max(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for evaluating the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    # outputs = two arrays (one array of size 4 for the hidden layer plus one array of size 10 for the output layer)\n",
    "    return outputs \n",
    "\n",
    "def predict(network, input):\n",
    "    \"\"\"run input through the network and return output of last layer\"\"\"\n",
    "    return feed_forward(network, input)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function for backpropagation that we'll need to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, target):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # compute the delta of the output values\n",
    "    output_deltas = [output * (1 - output) * (output - target[i]) # (1)\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]): # loop over weights of neurons in output layer\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]): # loop over output of neurons in hidden layer\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output # (2)\n",
    "\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]]) # (3)\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]): # loop over weights of neurons in hidden layer\n",
    "        for j, input in enumerate(input_vector + [1]): # loop over output of neurons in first layer, i.e. the inputs\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two lines marked with **(1)** and **(2)** we rediscover\n",
    "$$ \\Delta w_{ij} = - \\eta \\frac{\\partial E}{\\partial w_{ij}} = - (\\hat y - y) \\varphi'(\\text{net}_j) x_i,$$\n",
    "which was introduced in the [short introduction][1] on backpropagation. \n",
    "\n",
    "[1]: NN_Activation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **(1)** we compute $(\\hat y - y) \\varphi'(\\text{net}_j)$ with a learning rate $\\eta$ of $\\frac12$. The term $\\hat y - y$ is `output - target[i]` for each target output value (activation) of the neurons in the last layer (`outputs`) and the vector of target values (`target`), and $\\varphi'$ is the derivative of the sigmoid function that we use as activation function of the output layer, i.e. is $\\varphi'(x) = x(1-x)$ or `output * (1 - output)` in code.\n",
    "\n",
    "Finally, in **(2)** this is just multiplied by $x_i$ which is `hidden_output`, an entry in the vector of output values of the hidden layer (`hidden_outputs`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand equation **(3)** in the above code, which has not been spelled out in the [introduction][2], look again at the [Backpropagation Algorithm][1]:\n",
    "\n",
    "$$\\delta_j^k = g'(a_j^k) \\sum_{l=1}^{r^{k+1}} w_{jl}^{k+1}\\delta_l^{k+1}$$\n",
    "\n",
    "Here, $k$ is our (single) hidden layer and $k+1$ is the output layer, i.e. $a_j^k$ are the outputs of the hidden layer.\n",
    "$g'(x) = x(1-x)$ is again the derivative of the sigmoid. $r^{k+1}$ is the number of nodes in the output layer and the sum corresponds to the `dot` product multiplying the `output_deltas` ($\\delta_l^k+1$) and the weights `network[-1]` of the output layer ($w_{jl}^{k+1}$).\n",
    "\n",
    "[1]: https://brilliant.org/wiki/backpropagation/\n",
    "[2]: NN_Activation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stylized figures that will serve as inputs to train on (we only have one training set here with one input data per label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_digits = [\n",
    "   0, \"\"\"11111\n",
    "         1...1\n",
    "         1...1\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "   0, \"\"\".111.\n",
    "         1...1\n",
    "         1...1\n",
    "         1...1\n",
    "         .111.\"\"\",\n",
    "\n",
    "   1, \"\"\"..1..\n",
    "         ..1..\n",
    "         ..1..\n",
    "         ..1..\n",
    "         ..1..\"\"\",\n",
    "\n",
    "   1, \"\"\"..1..\n",
    "         .11..\n",
    "         1.1..\n",
    "         ..1..\n",
    "         ..1..\"\"\",\n",
    "\n",
    "   2, \"\"\"11111\n",
    "         ....1\n",
    "         11111\n",
    "         1....\n",
    "         11111\"\"\",\n",
    "\n",
    "   3, \"\"\"11111\n",
    "         ....1\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\",\n",
    "\n",
    "   3, \"\"\"1111.\n",
    "         ....1\n",
    "         .111.\n",
    "         ....1\n",
    "         1111.\"\"\",\n",
    "\n",
    "   4, \"\"\"1...1\n",
    "         1...1\n",
    "         11111\n",
    "         ....1\n",
    "         ....1\"\"\",\n",
    "\n",
    "   5, \"\"\"11111\n",
    "         1....\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\",\n",
    "\n",
    "   5, \"\"\"11111\n",
    "         1....\n",
    "         1111.\n",
    "         ....1\n",
    "         1111.\"\"\",\n",
    "\n",
    "   6, \"\"\"11111\n",
    "         1....\n",
    "         11111\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "   6, \"\"\".111\n",
    "         1....\n",
    "         1111.\n",
    "         1...1\n",
    "         .111.\"\"\",\n",
    "\n",
    "   7, \"\"\"11111\n",
    "         ....1\n",
    "         ....1\n",
    "         ....1\n",
    "         ....1\"\"\",\n",
    "\n",
    "   7, \"\"\"11111\n",
    "         ....1\n",
    "         ..1..\n",
    "         ..1..\n",
    "         ..1..\"\"\",\n",
    "\n",
    "   8, \"\"\"11111\n",
    "         1...1\n",
    "         11111\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "   8, \"\"\".111.\n",
    "         1...1\n",
    "         .111.\n",
    "         1...1\n",
    "         .111.\"\"\",\n",
    "\n",
    "   9, \"\"\"11111\n",
    "         1...1\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\"]\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs (pixel images) and targets (one-hot labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = list(map(make_digit, raw_digits[1::2]))\n",
    "labels  = raw_digits[0::2]\n",
    "targets = [[1 if i == j else 0 for i in range(10)]\n",
    "           for j in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network structure and initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)   # to get repeatable results\n",
    "input_size = 25  # each input is a vector of length 25 (25 pixels)\n",
    "num_hidden = 6   # number of neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "                for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "                for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the training using the backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,000 iterations seems enough to converge\n",
    "for x in range(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "    if x % 1000 == 0:\n",
    "        accuracy = sum([argmax(predict(network, input)) == label for input, label in zip(inputs, labels)])\n",
    "        print(\"Iterations done: %d, accuracy: %.2f\" % (x, accuracy / len(inputs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the probabilities of the labels the network predicts to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = []\n",
    "for i, input in enumerate(inputs):\n",
    "    outputs = predict(network, input)\n",
    "    print(i, [round(p,2) for p in outputs])\n",
    "    m.append(outputs)\n",
    "\n",
    "# This is not a confusion matrix.\n",
    "plt.imshow(m, plt.cm.Blues);\n",
    "plt.xlabel(\"Probability for label\")\n",
    "plt.ylabel(\"True label\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([round(x, 2) for x in\n",
    "      predict(network,\n",
    "                [0,1,1,1,0,    # .@@@.\n",
    "                 0,0,0,1,1,    # ...@@\n",
    "                 0,0,1,1,0,    # ..@@.\n",
    "                 0,0,0,1,1,    # ...@@\n",
    "                 0,1,1,1,0])]) # .@@@.\n",
    "\n",
    "print([round(x, 2) for x in\n",
    "      predict(network, \n",
    "                [0,1,1,1,0,    # .@@@.\n",
    "                 1,0,0,1,1,    # @..@@\n",
    "                 0,1,1,1,0,    # .@@@.\n",
    "                 1,0,0,1,1,    # @..@@\n",
    "                 0,1,1,1,0])]) # .@@@."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the weights the network has learned for each of the five hidden neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(x, y, hatch, color):\n",
    "    \"\"\"return a matplotlib 'patch' object with the specified\n",
    "    location, crosshatch pattern, and color\"\"\"\n",
    "    return matplotlib.patches.Rectangle((x - 0.5, y - 0.5), 1, 1,\n",
    "                                        hatch=hatch, fill=False, color=color)\n",
    "\n",
    "\n",
    "def show_weights(neuron_idx, ax):\n",
    "    weights = network[0][neuron_idx]\n",
    "\n",
    "    grid = [weights[row:(row+5)]      # turn the weights into a 5x5 grid\n",
    "            for row in range(0,25,5)] # [weights[0:5], ..., weights[20:25]]\n",
    "\n",
    "    pos = ax.imshow(grid,\n",
    "                    cmap=matplotlib.cm.coolwarm,\n",
    "                    interpolation='none', # plot blocks as blocks\n",
    "                    vmin = -8, vmax = 8) # define a unique range for all subplots\n",
    "    \n",
    "    # print bias\n",
    "    ax.set_xlabel(\"bias = %.2f\" % weights[25])\n",
    "    return pos\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3), ncols=num_hidden)\n",
    "for idx in range(num_hidden):\n",
    "    pos = show_weights(idx, ax[idx])\n",
    "    #fig.colorbar(pos, ax = ax[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(blue = large negative, red = large positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(output_layer, cmap=matplotlib.cm.coolwarm)\n",
    "plt.xlabel(\"Weight of hidden neuron (and bias)\")\n",
    "plt.ylabel(\"Output label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(blue = large negative, red = large positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it discriminates e.g. 0 and 8 or 5 and 9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
